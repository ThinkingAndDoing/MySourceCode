


Node.js 实现方式
https://segmentfault.com/a/1190000008745531
http://www.cnblogs.com/ryt103114/p/6085271.html
https://segmentfault.com/a/1190000003851057

Python爬虫在处理由Javascript动态生成的页面时有哪些解决方案？
js代码是需要js引擎运行的，Python只能通过HTTP请求获取到HTML、CSS、JS原始代码而已。
方法一：
一种 像大家说的 模拟浏览器环境 用一些库来执行这些js 相关的库大家都提到了
如果你直接运行页面上所有js（就像浏览器做的那样），然后获取最终的HTML DOM树，这样的性能非常地糟糕，不建议使用这样的方法。因为Python和js性能本身都很差，如果这样做，会消耗大量CPU资源并且最终只能获得极低的抓取效率。
刚才看了下，北邮人论坛页面也是比较复杂的，如果直接模拟浏览器解析，性能真的非常糟糕。
方法二：
还有一种 就是手动分析 这种我觉得是比较好的 js的作用的确很多 但是爬虫关注的是数据 js无非就是请求数据 或者请求回来以后再进行加工
总之 它的数据肯定也有来源的 如果是网络请求来的 我们也请求这个数据就好了 如果是自己生成的 同样代码模拟下就好了
只不过这种办法可能比较消耗时间
方法三：
还有一点 可以试试用nodejs跑js代码 当然前提是那个代码不涉及浏览器的一些内容 比如扣扣空间的密码加密代码 它就是将输入的代码加密成密文 这样我们可以把这段加密函数直接保存下来 爬虫时每次调用下就好 Node.js + co
写到这 越来越感叹 这js大一统的时代感觉在逼近啊
方法四：
JavaScript的执行需要js引擎，于是引入今天的主角PyV8。

上规模的整站爬取：
Python + Scrapy
如果说上面两个方案里DIY 的 spider是小米加步枪，那Scrapy简直就是重工加农炮，好用到不行，自定义爬取规则，http错误处理，XPath，RPC，Pipeline机制等等等。而且，由于Scrapy是基于Twisted实现的，所以同时兼顾有非常好的效率，相对来说唯一的缺点就是安装比较麻烦，依赖也比较多，我还算是比较新的osx，一样没办法直接pip install scrapy
另外如果在spider中引入xpath的话，再在chrome上安装xpath的插件，那么解析路径一目了然，开发效率奇高。






